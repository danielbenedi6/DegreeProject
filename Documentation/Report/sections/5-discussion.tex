\chapter{Discussion}
\label{chapter:discussiona}
We investigated how performance differ for different heuristics in a $k$-d tree. The results show that there is not a significant difference in performance between canonical, SAH, CCH, median of hyperplane with max variance and minimum variance union heuristics.

\section{Building performance}
Firstly, we needed to analyze how each heuristic affect to the time of building the $k$-d tree. We considered that if an heuristic takes more time to build in parallel than the canonical in serial it is not worth it, due to the great amount of time. In figures \ref{fig:density}, \ref{fig:neurons} and \ref{fig:extrapolation}, we can observe the difference between the building time for the different heuristics in the left side. In those figures, we can observe that obviously implementing everything with parallelization has sped up almost by a factor of 3. 

The figure \ref{fig:neurons} shows that the minimum variance union heuristic includes a lot of overhead and therefore it is not worth it. Similar happens with surface area heuristic. Although a $\mathcal{O}(n\log{}^2(n)$ has been implemented, the memory usage was so high than anything above 150 neurons were over 16 GiB and therefore this implementation was unfeasible. In the other hand, curve complexity heuristic and median of hyperplane with maximum variance have a lower cost than the serial canonical splitting what means that they could be feasible to use in a real case.

\section{Querying performance}
Once the tree has been built, we were able to query it looking for all touchpoints close to a given point in our reconstruction of the neuronal network. We have observed in figures  \ref{fig:density} and \ref{fig:neurons} (middle figure) that all the heuristics performed the same . There were no significant differences in the performance. We firstly believed it is since the points are shown sparse that the heuristics do not have any effect, the number of touchpoints can be observed in the right figures in \ref{fig:density} and \ref{fig:neurons}. To prove this fact, we repeated the experiments increasing the density. The results in the figure \ref{fig:extrapolation} show that although the amount of touchpoints is higher, there is no better performance of the $k$-d trees built with the heuristics.

Theoretically, we know that the worst-case for querying is $\mathcal{O}(n)$, but on average it should be $\mathcal{O}(log(n))$. Experimentally, we saw that our results fit better to a linear cost rather than a logarithmic cost. Tables \ref{tab:density}, \ref{tab:neurons} and \ref{tab:extrapolation} shows that the lineal function fits with a lower $R^2$ (coefficient of determination) and it is also evident in the figures \ref{fig:approx_density}, \ref{fig:approx_neurons} and \ref{fig:approx_extrapolation}. Analysing the reasons for the lack of improvement in the performance, we ended up with three possible reasons. The implemented algorithm to perform the touchpoint query (algorithm \ref{alg:find}) could be improved if in lines \ref{alg:find:begin1} to \ref{alg:find:end1} and \ref{alg:find:begin2} to \ref{alg:find:end2} another condition is added where those lines are only executed if the distance in that dimension between the root and the query point is less or equal to the threshold distance. Another reason could be if the distance between the query point and the data is far, the number of points examined could be upper bounded by $2^n$ \cite{Friedman1977-bv}. This led to what we believed to be the main reason. At the beginning of this thesis, we decided to create one $k$-d tree per neuron. What we believed to be an improvement because we will examine fewer points per neuron has turned out to be a flag because for some trees the query point is so far that we need to examine all the trees. A solution to this problem could be using a lower amount of $k$-d trees such as 1 to 5 and changing the query algorithm from the nearest neighbour to a range search.